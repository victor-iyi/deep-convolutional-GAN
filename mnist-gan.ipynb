{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Convolutional Generative Adversarial Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "data = input_data.read_data_sets('data/MNIST/', one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Training:   {:,}'.format(data.train.num_examples))\n",
    "print('Testing:    {:,}'.format(data.test.num_examples))\n",
    "print('Validation:  {:,}'.format(data.validation.num_examples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Inputs\n",
    "image_size = 28\n",
    "image_channel = 1\n",
    "image_shape = (image_size, image_size, image_channel)\n",
    "image_size_flat = image_size * image_size * image_channel\n",
    "num_classes = 10\n",
    "\n",
    "# Network\n",
    "filter_size = 5\n",
    "hidden1_filter = 32\n",
    "hidden2_filter = 64\n",
    "fc1_size = 1024\n",
    "fc2_size = 1\n",
    "dropout = 0.8\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `weights` and `biases`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weight(shape):\n",
    "    initial = tf.truncated_normal(shape=shape, mean=0, stddev=0.02)\n",
    "    return tf.Variable(initial, name='weight')\n",
    "\n",
    "\n",
    "def bias(shape):\n",
    "    initial = tf.zeros(shape=[shape])\n",
    "    return tf.Variable(initial, name='bias')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `convolution` and `pooling` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv2d(X, W, strides=[1, 1, 1, 1]):\n",
    "    return tf.nn.conv2d(X, W, strides=strides, padding='SAME')\n",
    "\n",
    "\n",
    "def max_pool(X):\n",
    "    return tf.nn.max_pool(X, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `flatten` layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def flatten(layer):\n",
    "    layer_shape = layer.get_shape()\n",
    "    num_features = np.array(layer_shape[1:4], dtype=int).prod()\n",
    "    layer_flat = tf.reshape(layer, [-1, num_features])\n",
    "    return layer_flat, num_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convolutional layer\n",
    "def conv_layer(prev_layer, prev_filter, layer_filter, layer_name, \n",
    "               strides=[1, 1, 1, 1], filter_size=5, use_pool=True, batch_norm=False):\n",
    "    with tf.name_scope(layer_name):\n",
    "        W = weight(shape=[filter_size, filter_size, prev_filter, layer_filter])\n",
    "        b = bias(shape=layer_filter)\n",
    "        layer = conv2d(prev_layer, W, strides=strides) + b\n",
    "        if batch_norm:\n",
    "            layer = tf.contrib.layers.batch_norm(layer, epsilon=1e-3, scope='batch_norm')\n",
    "        if use_pool:\n",
    "            layer = max_pool(layer)\n",
    "        layer = tf.nn.relu(layer)\n",
    "        return layer\n",
    "\n",
    "\n",
    "# fully connected layer\n",
    "def fc_layer(prev_layer, prev_size, layer_size, layer_name, \n",
    "             use_relu=True, dropout=False, batch_norm=False):\n",
    "    with tf.name_scope(layer_name):\n",
    "        W = weight(shape=[prev_size, layer_size])\n",
    "        b = bias(shape=layer_size)\n",
    "        layer = tf.matmul(prev_layer, W) + b\n",
    "        # Use batch normalization\n",
    "        if batch_norm:\n",
    "            layer = tf.contrib.layers.batch_norm(layer, epsilon=1e-5, scope='batch_norm')\n",
    "        # Use ReLU\n",
    "        if use_relu:\n",
    "            layer = tf.nn.relu(layer)\n",
    "        # Add dropout\n",
    "        if dropout:\n",
    "            layer = tf.nn.dropout(layer, keep_prob)\n",
    "        return layer\n",
    "\n",
    "\n",
    "# Output layer\n",
    "def output_layer(fc_layer, fc_size, num_classes):\n",
    "    with tf.name_scope('output_layer'):\n",
    "        W = weight(shape=[fc_size, num_classes])\n",
    "        b = bias(shape=num_classes)\n",
    "        logits = tf.matmul(fc_layer, W) + b\n",
    "        y_pred = tf.nn.softmax(logits)\n",
    "        y_pred_true = tf.argmax(y_pred, axis=1)\n",
    "        return logits, y_pred_true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discrimitor (Convolutional NN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def discriminator(X_image, reuse=False):\n",
    "    if reuse:\n",
    "        tf.get_variable_scope().reuse_variables()\n",
    "    \n",
    "    # Convolutional blocks\n",
    "    hidden1 = conv_layer(X_image, image_channel, hidden1_filter, 'dis_hidden1', use_pool=True)\n",
    "    hidden2 = conv_layer(hidden1, hidden1_filter, hidden2_filter, 'dis_hidden2', use_pool=True)\n",
    "    \n",
    "    # Fully connected blocks\n",
    "    hidden2_flat, hidden2_flat_filters = flatten(hidden2)\n",
    "    fc1_layer = fc_layer(hidden2_flat, hidden2_flat_filters, fc1_size, 'dis_fc1', use_relu=True)\n",
    "    fc2_layer = fc_layer(fc1_layer, fc1_size, fc2_size, 'dis_fc2', use_relu=False)\n",
    "    \n",
    "    return fc2_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator (Deconvolutional NN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generator(batch_size, z_dim, up_scale=56):\n",
    "    z = tf.truncated_normal(shape=[batch_size, z_dim], mean=0, stddev=1, name='z')\n",
    "    scale = up_scale * up_scale\n",
    "    \n",
    "    # Fully connected block\n",
    "    fc1_layer = fc_layer(z, z_dim, scale, 'gen_fc1', use_relu=True, batch_norm=True)\n",
    "    fc1_layer = tf.reshape(fc1_layer, [-1, up_scale, up_scale, image_channel])\n",
    "    \n",
    "    # Convolutional block\n",
    "    hidden1 = conv_layer(fc1_layer, image_channel, z_dim/2, 'gen_deconv1', \n",
    "                         strides=[1, 2, 2, 1], filter_size=3, use_pool=False, batch_norm=True)\n",
    "    hidden1 = tf.image.resize_images(hidden1, size=[up_scale, up_scale])\n",
    "    \n",
    "    hidden2 = conv_layer(hidden1, z_dim/2, z_dim/4, 'gen_deconv2', \n",
    "                         strides=[1, 2, 2, 1], filter_size=3, use_pool=False, batch_norm=True)\n",
    "    hidden2 = tf.image.resize_images(hidden2, size=[up_scale, up_scale])\n",
    "    \n",
    "    hidden3 = conv_layer(hidden2, z_dim/4, 1, 'gen_deconv3', \n",
    "                         strides=[1, 2, 2, 1], filter_size=1, use_pool=False, batch_norm=False)\n",
    "    hidden3 = tf.nn.sigmoid(hidden3)\n",
    "    # No batch normalization in the final layer but we add sigmoid activation \n",
    "    # to make the generated images more compact/crisper.\n",
    "    return hidden3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "\n",
    "batch_size = 50\n",
    "z_dimensions = 100\n",
    "\n",
    "X_placeholder = tf.placeholder('float', shape=[None, 28, 28, 1], name='X_placeholder')\n",
    "\n",
    "Gz = generator(batch_size, z_dimensions)  # the generated image\n",
    "Dx = discriminator(X_placeholder)   # classify the real image\n",
    "Dg = discriminator(Gz, reuse=True)  # classify the generated image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function\n",
    "#### generator's loss\n",
    "The generator wants to optimize the probability that **the generated image is rated highly** i.e _for every image it generates, the discriminator should say it's real_\n",
    "\n",
    "#### discriminator's loss\n",
    "The discriminator wants to:\n",
    "* optimize the probability that **the real data is rated highly**\n",
    "* optimize the probability that **the generated data is rated poorly**\n",
    "That is, when it classifies images from the training data, it should say this is the real image and when it classifies images from the generated data, it should say this is the fake image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# generator's loss\n",
    "g_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=Dg, labels=tf.ones_like(Dg)))\n",
    "# discriminator's loss\n",
    "d_loss_real = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=Dx, labels=tf.ones_like(Dx)))\n",
    "d_loss_fake = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=Dg, labels=tf.zeros_like(Dg)))\n",
    "d_loss = d_loss_real + d_loss_fake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
